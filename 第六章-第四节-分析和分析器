	分析过程由以下步骤组成：
	首先，将一块文本分词为单独的短语，方便在反向索引中使用。
	然后将这些短语标准化为基本格式来提升它们的可搜索性。
	这个过程是由分析器完成的。一个分析器是一个将以下三个方法联合在一起的包装器：
	特性过滤器：
	首先，字串被依次传入到多个特性过滤器中。它们的作用是在分词前整理输入的字串。一个特性过滤器可用来将HTML去除，或者将&字符转换成单词and。
	分词器：
	接下来，字符串被分词器分割成一个一个的短语。一个简单的分词器可能只是在遇到空格或者标点符号的时候将文本进行分割。
	标记过滤器：
	最后，每个短语都被依次传入到多个标记过滤器中，它们可以修改短语，比如小写话，移除一些短语，比如a,and,the之类的语气词或者添加短语，比如像jump 和 leap 之类的同义词。
	Elasticsearch提供了许多可以直接使用的特性过滤器，分词器和标记过滤器。这些工具可以被联合起来创建自定义分析器来满足不同的需求。接下来的章节会详细讨论。
	但是，Elasticsearch也提供可直接使用的内置的分析器，接下来会列出比较重要的一些并且显示它们之间的不同行为。我们将会展示经过它们分析之后的结果：
	"Set the shape to semi-transparent by calling set_trans(5)"
	基本分析器：
	基本分析器就是默认的分析器，这是对于所有语言来说最佳的普适选择。它将文本按照字边界分割，字边界在Unicode中定义,移除大多数的标点符号，最后，在将所有的短语小写，产出如下：
	set, the, shape, to, semi, transparent, by, calling, set_trans, 5
	简单分析器：
	简单分析器将所有不是字母的字符当作是分割符，并且将所有短语小写，产出如下：
	set, the, shape, to, semi, transparent, by, calling, set, trans
	空格分析器：
	空格分析器将空格作为分割符并且不会将短语小写，产出如下：
	Set, the, shape, to, semi-transparent, by, calling, set_trans(5)
	语言分析器：
	语言分析器支持多种语言（不包括中文）。它们支持将指定语言的特性转换成目录。举个例子，英语分析器中有一系列的英语无用词（比如and\the之类对关联性影响甚微的词），这是会在分词过程中被丢弃的。这个分析器也支持将英语单词短语化，因为它可以理解英语的语法。
	英语分析器会有如下产出：
	set, shape, semi, transpar, call, set_tran, 5
	注意transparent,calling和set_trans都被还原为词根了。
	当我们索引一个文档的时候，它的全文本字段会被分析成用于创建反响索引的短语，但是，当我们在一个全文本字段上搜索的时候，我们需要通过相同的分析过程把查询字串发送给Elasticsearch，这样才能保证我们搜索的短语和索引中的短语是一个格式的。
	全文本查询将会在后面讨论，现在先理解每个字段是怎么定义的，这样才能使它们表现出正确的性状：
	当你查询一个全文本字段的时候，查询会对查询字串应用与分词相同的分析器来创建正确的可用于搜索的短语。
	当你查询一个确定值字段时，查询不会分析查询字段，而是直接搜索你指定的确定值。
	现在你应该可以理解为什么我们展示在第一节的查询为什么会返回这些结果了吧：
	日期字段包含了一个确定值，即2014-09-15,所以搜索在这个字段上搜索2014没有匹配结果。
	_all字段是一个全文本字段，所以分析过程将日期转换为3个短语：2014，09,15。
	如果你才刚开始使用Elasticsearch，可能要理解那些字段被分词过并且存储在索引中是一件比较困难的事。为了提高对这个过程的理解，你可以使用anlyze API来查看文本是怎么被分析的。通过在查询字串中的参数指定使用何种分析器，以及指定需要被分析的文本。
	GET /_analyze?analyzer=standard
	Text to analyze
	结果中的每个元素表示一个单独的短语：
	{
	   "tokens": [
	      {
	         "token":        "text",
	         "start_offset": 0,
	         "end_offset":   4,
	         "type":         "<ALPHANUM>",
	         "position":     1
	      },
	      {
	         "token":        "to",
	         "start_offset": 5,
	         "end_offset":   7,
	         "type":         "<ALPHANUM>",
	         "position":     2
	      },
	      {
	         "token":        "analyze",
	         "start_offset": 8,
	         "end_offset":   15,
	         "type":         "<ALPHANUM>",
	         "position":     3
	      }
	   ]
	}
	token字段的值是实际上被存储到索引中的短语。position字段表明短语在原文中出现的顺序。start_offset和end_offset表明原始单词在文档中占据的字符位置。type字段中的值，比如<ALPHANUM>根据不同的分析器值也不尽相同，可以忽略，它们只在Elasticsearch的标识过滤器中被使用。
	当Elasticsearch检测到文档中的一个新的字串字段时，它会自动将这个字段当作是一个全文本字符串字段并且用基本分析器分析它。
	你可能会不想总是接受这种设置，比如换一种分析器之类。有时你可能还会想要一个字符串字段表现的就是一个字符串字段，按照你传递的值进行索引，不使用任何分析，比如字串型的用户ID或者内部使用的状态字段。
	下一节将会介绍如何手动设置它们。
	
